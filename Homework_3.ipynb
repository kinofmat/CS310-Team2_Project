{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kinofmat/CS310-Team2_Project/blob/main/Homework_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owQ_FPfHEIOY"
      },
      "source": [
        "# Problem Set 3\n",
        "In this problem set you will get some practice with gradient descent and sub-gradient descent. There is a template for setting up your code and plotting your results at the end of this notebook, which you may find useful. Note that it is only a suggestion -- you are not required to use it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY6Ysm1TELWc"
      },
      "source": [
        "## Problem 1: Gradient Descent\n",
        "Consider three quadratics, as given by:\n",
        "$$\n",
        "f_i(x) = \\frac{1}{2} x^{\\top}Q_ix + q_i^{\\top} x + c_i,\n",
        "$$\n",
        "for the three triples $(Q_i,q_i,c_i)$, $i=1,2,3$ given below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqMCBxayEMT4"
      },
      "source": [
        "import numpy as np\n",
        "Q1 = np.array([[ 1.17091573, -0.03686123, -0.1001259 ],\n",
        "       [-0.03686123,  1.03835691,  0.17285956],\n",
        "       [-0.1001259 ,  0.17285956,  1.06072736]])\n",
        "Q2 = np.array([[ 15.27095759,  -1.97915834, -14.22190153],\n",
        "       [ -1.97915834,   0.34660759,   1.91586927],\n",
        "       [-14.22190153,   1.91586927,  15.76943482]])\n",
        "Q3 = np.array([[28.59657006,  0.3684004 ,  0.90750259],\n",
        "       [ 0.3684004 , 28.11480924,  0.81866989],\n",
        "       [ 0.90750259,  0.81866989, 28.7886207 ]])\n",
        "q1 = np.array([-4.68686663, -0.89027884, -1.57340281])\n",
        "q2 = np.array([ 6.75973216,  1.23223936, -0.87956749])\n",
        "q3 = np.array([ 0.8684369 , -4.69970837, -1.09690726])\n",
        "c1 = 1.61888566;\n",
        "c2 = -2.66426693;\n",
        "c3 = 0.84184738;\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4pg_ZIgESeA"
      },
      "source": [
        "### Part (A)\n",
        "Implement gradient descent with a step-size of $0.1$ for all three quadratics, and plot function value versus iteration number. Explain what you see."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oNV2Pp-EWKH"
      },
      "source": [
        "### Part (B)\n",
        "Find a step-size for each quadratic (it can be different for each), and implement gradient descent with this stepsize. Plot function value versus iteration number. Try to pick a step size that gives you fast convergence. How did you pick it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXGLnr8zEYKu"
      },
      "source": [
        "### Part (C)\n",
        "Even with your carefully tuned choice of step size, you will see that the rates of convergence are not the same, Explain why this is the case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1iEjab2EclQ"
      },
      "source": [
        "### Part (D) -- Optional --\n",
        "Read about back tracking line search (BTLS) and implement it here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh1qRKIlElT_"
      },
      "source": [
        "## Problem 2: Sub-gradient Method\n",
        "Consider a least squares problem with $\\ell^1$ regularization:\n",
        "$$\n",
        "\\min_x \\left[f(x) = \\frac{1}{2}\\|{Ax-b}\\|_2^2 + \\lambda \\|{x}\\|_1 \\right]\n",
        "$$\n",
        "\n",
        "This problem is often called LASSO (least absolute shrinkage and selection operator) and is known to induce {\\em sparse} solutions with few nonzero elements in $x$, which can have advantages in terms of computation and interpretability. This problem is nonsmooth due to the regularization term. It is also not strongly convex when $A$ has more columns than rows. We (i.e., you) will solve this problem using several different algorithms in this class. We start with what we have seen thus far: the subgradient method.\n",
        "\n",
        "The dataset represented in the matrices provided in the numpy binary files A.npy and b.npy are from a diabetes dataset (scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html) with 10 features that has been corrupted with an additional 90 noisy features. Thus a sparse solution should be very effective. Below you will find some skeleton code to help with loading the data, running the algorithm and plotting the results. Don't use stock optimization code, you should develop the core part of this assignment yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlwKNrNcEp7F"
      },
      "source": [
        "Minimize $f(x)$ using $10^4$ iterations of the subgradient method starting with $t=0$ and $x_0 =0$.\n",
        "\n",
        "### Part (A)\n",
        "Use a decreasing step size of $\\eta_t = c/t$ with values for $c$ that (roughly) optimize the empirical performance. Separately record the (unsquared) error $\\|Ax_t-b\\|$ and the regularization term $\\|x\\|_1$.\n",
        "\n",
        "### Part (B)\n",
        "Now use a more slowly decreasing step size of $\\eta_t = c/\\sqrt{t+1}$ with values for $c$ that (roughly) optimize the empirical performance. Separately record the (unsquared) error $\\|Ax_t-b\\|$ and the regularization term $\\|x\\|_1$.\n",
        "\n",
        "### Part (C)\n",
        "Now try to find the best fixed step size. Plot the results and compare to the decreasing step size you see above.\n",
        "\n",
        "### Part (D) -- Optional --\n",
        "If you did the BTLS optional part above, either implement BTLS for subgradient method, or explain why the challenges are with this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgEPG09USxT8"
      },
      "source": [
        "Here is a template that may be useful. Note that it is designed for the LASSO problem, so you will have to modify it to use it for the quadratic problems at the beginning, but the ideas are the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU1ReScpEqks"
      },
      "source": [
        "from __future__ import division\n",
        "import numpy as np\n",
        "import numpy.random as rn\n",
        "import numpy.linalg as la\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "\n",
        "def subgradient(x, A, b, t, lam, c=1e-5):\n",
        "    # update x (your code here), set c above\n",
        "    return x\n",
        "\n",
        "def descent(update, A, b, reg, T=int(1e4)):\n",
        "    x = np.zeros(A.shape[1])\n",
        "    error = []\n",
        "    l1 = []\n",
        "    for t in range(T):\n",
        "        # update A (here subgradient, but you can re-use when we develop other algorithms)\n",
        "        x = update(x, A, b, t, reg)\n",
        "\n",
        "        # record error and l1 norm\n",
        "        if (t % 1 == 0) or (t == T - 1):\n",
        "            error.append(la.norm(np.dot(A, x) - b))\n",
        "            l1.append(np.sum(abs(x)))\n",
        "\n",
        "            assert not np.isnan(error[-1])\n",
        "\n",
        "    return x, error, l1\n",
        "\n",
        "\n",
        "def main(T=int(1e3)):\n",
        "    A = np.load(\"A.npy\")\n",
        "    b = np.load(\"b.npy\")\n",
        "\n",
        "    # modify regularization parameters below\n",
        "    x_sg, error_sg, l1_sg = descent(subgradient, A, b, reg=0., T=T)\n",
        "\n",
        "    plt.clf()\n",
        "    plt.plot(error_sg, label='Subgradient')\n",
        "    plt.title('Error')\n",
        "    plt.legend()\n",
        "    plt.savefig('error.eps')\n",
        "\n",
        "    plt.clf()\n",
        "    plt.plot(l1_sg, label='Subgradient')\n",
        "    plt.title(\"$\\ell^1$ Norm\")\n",
        "    plt.legend()\n",
        "    plt.savefig('l1.eps')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}